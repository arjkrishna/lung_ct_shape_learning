{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import easy_tf_log\n",
    "import numpy as np\n",
    "from numpy.testing import assert_equal\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import RunningStat, batch_iter\n",
    "\n",
    "\n",
    "class RewardPredictorEnsemble:\n",
    "    \"\"\"\n",
    "    An ensemble of reward predictors and associated helper functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cluster_job_name,\n",
    "                 core_network,\n",
    "                 lr=1e-4,\n",
    "                 cluster_dict=None,\n",
    "                 batchnorm=False,\n",
    "                 dropout=0.0,\n",
    "                 n_preds=1,\n",
    "                 log_dir=None):\n",
    "        self.n_preds = n_preds\n",
    "        graph, self.sess = self.init_sess(cluster_dict, cluster_job_name)\n",
    "        # Why not just use soft device placement? With soft placement,\n",
    "        # if we have a bug which prevents an operation being placed on the GPU\n",
    "        # (e.g. we're using uint8s for operations that the GPU can't do),\n",
    "        # then TensorFlow will be silent and just place the operation on a CPU.\n",
    "        # Instead, we want to say: if there's a GPU present, definitely try and\n",
    "        # put things on the GPU. If it fails, tell us!\n",
    "        if tf.test.gpu_device_name():\n",
    "            worker_device = \"/job:{}/task:0/gpu:0\".format(cluster_job_name)\n",
    "        else:\n",
    "            worker_device = \"/job:{}/task:0\".format(cluster_job_name)\n",
    "        device_setter = tf.train.replica_device_setter(\n",
    "            cluster=cluster_dict,\n",
    "            ps_device=\"/job:ps/task:0\",\n",
    "            worker_device=worker_device)\n",
    "        self.rps = []\n",
    "        with graph.as_default():\n",
    "            for pred_n in range(n_preds):\n",
    "                with tf.device(device_setter):\n",
    "                    with tf.variable_scope(\"pred_{}\".format(pred_n)):\n",
    "                        rp = RewardPredictorNetwork(\n",
    "                            core_network=core_network,\n",
    "                            dropout=dropout,\n",
    "                            batchnorm=batchnorm,\n",
    "                            lr=lr)\n",
    "                self.rps.append(rp)\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            # Why save_relative_paths=True?\n",
    "            # So that the plain-text 'checkpoint' file written uses relative paths,\n",
    "            # which seems to be needed in order to avoid confusing saver.restore()\n",
    "            # when restoring from FloydHub runs.\n",
    "            self.saver = tf.train.Saver(max_to_keep=1, save_relative_paths=True)\n",
    "            self.summaries = self.add_summary_ops()\n",
    "\n",
    "        self.checkpoint_file = osp.join(log_dir,\n",
    "                                        'reward_predictor_checkpoints',\n",
    "                                        'reward_predictor.ckpt')\n",
    "        self.train_writer = tf.summary.FileWriter(\n",
    "            osp.join(log_dir, 'reward_predictor', 'train'), flush_secs=5)\n",
    "        self.test_writer = tf.summary.FileWriter(\n",
    "            osp.join(log_dir, 'reward_predictor', 'test'), flush_secs=5)\n",
    "\n",
    "        self.n_steps = 0\n",
    "        self.r_norm = RunningStat(shape=n_preds)\n",
    "\n",
    "        misc_logs_dir = osp.join(log_dir, 'reward_predictor', 'misc')\n",
    "        easy_tf_log.set_dir(misc_logs_dir)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_sess(cluster_dict, cluster_job_name):\n",
    "        graph = tf.Graph()\n",
    "        cluster = tf.train.ClusterSpec(cluster_dict)\n",
    "        config = tf.ConfigProto(gpu_options={'allow_growth': True})\n",
    "        server = tf.train.Server(cluster, job_name=cluster_job_name, config=config)\n",
    "        sess = tf.Session(server.target, graph)\n",
    "        return graph, sess\n",
    "\n",
    "    def add_summary_ops(self):\n",
    "        summary_ops = []\n",
    "\n",
    "        for pred_n, rp in enumerate(self.rps):\n",
    "            name = 'reward_predictor_accuracy_{}'.format(pred_n)\n",
    "            op = tf.summary.scalar(name, rp.accuracy)\n",
    "            summary_ops.append(op)\n",
    "            name = 'reward_predictor_loss_{}'.format(pred_n)\n",
    "            op = tf.summary.scalar(name, rp.loss)\n",
    "            summary_ops.append(op)\n",
    "\n",
    "        mean_accuracy = tf.reduce_mean([rp.accuracy for rp in self.rps])\n",
    "        op = tf.summary.scalar('reward_predictor_accuracy_mean', mean_accuracy)\n",
    "        summary_ops.append(op)\n",
    "\n",
    "        mean_loss = tf.reduce_mean([rp.loss for rp in self.rps])\n",
    "        op = tf.summary.scalar('reward_predictor_loss_mean', mean_loss)\n",
    "        summary_ops.append(op)\n",
    "\n",
    "        summaries = tf.summary.merge(summary_ops)\n",
    "\n",
    "        return summaries\n",
    "\n",
    "    def init_network(self, load_ckpt_dir=None):\n",
    "        if load_ckpt_dir:\n",
    "            ckpt_file = tf.train.latest_checkpoint(load_ckpt_dir)\n",
    "            if ckpt_file is None:\n",
    "                msg = \"No reward predictor checkpoint found in '{}'\".format(\n",
    "                    load_ckpt_dir)\n",
    "                raise FileNotFoundError(msg)\n",
    "            self.saver.restore(self.sess, ckpt_file)\n",
    "            print(\"Loaded reward predictor checkpoint from '{}'\".format(ckpt_file))\n",
    "        else:\n",
    "            self.sess.run(self.init_op)\n",
    "\n",
    "    def save(self):\n",
    "        ckpt_name = self.saver.save(self.sess,\n",
    "                                    self.checkpoint_file,\n",
    "                                    self.n_steps)\n",
    "        print(\"Saved reward predictor checkpoint to '{}'\".format(ckpt_name))\n",
    "\n",
    "    def raw_rewards(self, obs):\n",
    "        \"\"\"\n",
    "        Return (unnormalized) reward for each frame of a single segment\n",
    "        from each member of the ensemble.\n",
    "        \"\"\"\n",
    "        assert_equal(obs.shape[1:], (256, 256))\n",
    "        n_steps = obs.shape[0]\n",
    "        feed_dict = {}\n",
    "        for rp in self.rps:\n",
    "            feed_dict[rp.training] = False\n",
    "            feed_dict[rp.s1] = [obs]\n",
    "        # This will return nested lists of sizes n_preds x 1 x nsteps\n",
    "        # (x 1 because of the batch size of 1)\n",
    "        rs = self.sess.run([rp.r1 for rp in self.rps], feed_dict)\n",
    "        rs = np.array(rs)\n",
    "        # Get rid of the extra x 1 dimension\n",
    "        rs = rs[:, 0, :]\n",
    "        assert_equal(rs.shape, (self.n_preds, n_steps))\n",
    "        return rs\n",
    "\n",
    "    def reward(self, obs):\n",
    "        \"\"\"\n",
    "        Return (normalized) reward for each frame of a single segment.\n",
    "\n",
    "        (Normalization involves normalizing the rewards from each member of the\n",
    "        ensemble separately, then averaging the resulting rewards across all\n",
    "        ensemble members.)\n",
    "        \"\"\"\n",
    "        assert_equal(obs.shape[1:], (256, 256))\n",
    "        n_steps = obs.shape[0]\n",
    "\n",
    "        # Get unnormalized rewards\n",
    "\n",
    "        ensemble_rs = self.raw_rewards(obs)\n",
    "        logging.debug(\"Unnormalized rewards:\\n%s\", ensemble_rs)\n",
    "\n",
    "        # Normalize rewards\n",
    "\n",
    "        # Note that we implement this here instead of in the network itself\n",
    "        # because:\n",
    "        # * It's simpler not to do it in TensorFlow\n",
    "        # * Preference prediction doesn't need normalized rewards. Only\n",
    "        #   rewards sent to the the RL algorithm need to be normalized.\n",
    "        #   So we can save on computation.\n",
    "\n",
    "        # Page 4:\n",
    "        # \"We normalized the rewards produced by r^ to have zero mean and\n",
    "        #  constant standard deviation.\"\n",
    "        # Page 15: (Atari)\n",
    "        # \"Since the reward predictor is ultimately used to compare two sums\n",
    "        #  over timesteps, its scale is arbitrary, and we normalize it to have\n",
    "        #  a standard deviation of 0.05\"\n",
    "        # Page 5:\n",
    "        # \"The estimate r^ is defined by independently normalizing each of\n",
    "        #  these predictors...\"\n",
    "\n",
    "        # We want to keep track of running mean/stddev for each member of the\n",
    "        # ensemble separately, so we have to be a little careful here.\n",
    "        assert_equal(ensemble_rs.shape, (self.n_preds, n_steps))\n",
    "        ensemble_rs = ensemble_rs.transpose()\n",
    "        assert_equal(ensemble_rs.shape, (n_steps, self.n_preds))\n",
    "        for ensemble_rs_step in ensemble_rs:\n",
    "            self.r_norm.push(ensemble_rs_step)\n",
    "        ensemble_rs -= self.r_norm.mean\n",
    "        ensemble_rs /= (self.r_norm.std + 1e-12)\n",
    "        ensemble_rs *= 0.05\n",
    "        ensemble_rs = ensemble_rs.transpose()\n",
    "        assert_equal(ensemble_rs.shape, (self.n_preds, n_steps))\n",
    "        logging.debug(\"Reward mean/stddev:\\n%s %s\",\n",
    "                      self.r_norm.mean,\n",
    "                      self.r_norm.std)\n",
    "        logging.debug(\"Normalized rewards:\\n%s\", ensemble_rs)\n",
    "\n",
    "        # \"...and then averaging the results.\"\n",
    "        rs = np.mean(ensemble_rs, axis=0)\n",
    "        assert_equal(rs.shape, (n_steps, ))\n",
    "        logging.debug(\"After ensemble averaging:\\n%s\", rs)\n",
    "\n",
    "        return rs\n",
    "\n",
    "    def preferences(self, s1s, s2s):\n",
    "        \"\"\"\n",
    "        Predict probability of human preferring one segment over another\n",
    "        for each segment in the supplied batch of segment pairs.\n",
    "        \"\"\"\n",
    "        feed_dict = {}\n",
    "        for rp in self.rps:\n",
    "            feed_dict[rp.s1] = s1s\n",
    "            feed_dict[rp.s2] = s2s\n",
    "            feed_dict[rp.training] = False\n",
    "        preds = self.sess.run([rp.pred for rp in self.rps], feed_dict)\n",
    "        return preds\n",
    "\n",
    "    def train(self, prefs_train, prefs_val, val_interval):\n",
    "        \"\"\"\n",
    "        Train all ensemble members for one epoch.\n",
    "        \"\"\"\n",
    "        print(\"Training/testing with %d/%d preferences\" % (len(prefs_train),\n",
    "                                                           len(prefs_val)))\n",
    "\n",
    "        start_steps = self.n_steps\n",
    "        start_time = time.time()\n",
    "\n",
    "        for _, batch in enumerate(batch_iter(prefs_train.prefs,\n",
    "                                             batch_size=32,\n",
    "                                             shuffle=True)):\n",
    "            self.train_step(batch, prefs_train)\n",
    "            self.n_steps += 1\n",
    "\n",
    "            if self.n_steps and self.n_steps % val_interval == 0:\n",
    "                self.val_step(prefs_val)\n",
    "\n",
    "        end_time = time.time()\n",
    "        end_steps = self.n_steps\n",
    "        rate = (end_steps - start_steps) / (end_time - start_time)\n",
    "        easy_tf_log.tflog('reward_predictor_training_steps_per_second',\n",
    "                          rate)\n",
    "\n",
    "    def train_step(self, batch, prefs_train):\n",
    "        s1s = [prefs_train.segments[k1] for k1, k2, pref, in batch]\n",
    "        s2s = [prefs_train.segments[k2] for k1, k2, pref, in batch]\n",
    "        prefs = [pref for k1, k2, pref, in batch]\n",
    "        feed_dict = {}\n",
    "        for rp in self.rps:\n",
    "            feed_dict[rp.s1] = s1s\n",
    "            feed_dict[rp.s2] = s2s\n",
    "            feed_dict[rp.pref] = prefs\n",
    "            feed_dict[rp.training] = True\n",
    "        ops = [self.summaries, [rp.train for rp in self.rps]]\n",
    "        summaries, _ = self.sess.run(ops, feed_dict)\n",
    "        self.train_writer.add_summary(summaries, self.n_steps)\n",
    "\n",
    "    def val_step(self, prefs_val):\n",
    "        val_batch_size = 32\n",
    "        if len(prefs_val) <= val_batch_size:\n",
    "            batch = prefs_val.prefs\n",
    "        else:\n",
    "            idxs = np.random.choice(len(prefs_val.prefs),\n",
    "                                    val_batch_size,\n",
    "                                    replace=False)\n",
    "            batch = [prefs_val.prefs[i] for i in idxs]\n",
    "        s1s = [prefs_val.segments[k1] for k1, k2, pref, in batch]\n",
    "        s2s = [prefs_val.segments[k2] for k1, k2, pref, in batch]\n",
    "        prefs = [pref for k1, k2, pref, in batch]\n",
    "        feed_dict = {}\n",
    "        for rp in self.rps:\n",
    "            feed_dict[rp.s1] = s1s\n",
    "            feed_dict[rp.s2] = s2s\n",
    "            feed_dict[rp.pref] = prefs\n",
    "            feed_dict[rp.training] = False\n",
    "        summaries = self.sess.run(self.summaries, feed_dict)\n",
    "        self.test_writer.add_summary(summaries, self.n_steps)\n",
    "\n",
    "\n",
    "class RewardPredictorNetwork:\n",
    "    \"\"\"\n",
    "    Predict the reward that a human would assign to each frame of\n",
    "    the input trajectory, trained using the human's preferences between\n",
    "    pairs of trajectories.\n",
    "\n",
    "    Network inputs:\n",
    "    - s1/s2     Trajectory pairs\n",
    "    - pref      Preferences between each pair of trajectories\n",
    "    Network outputs:\n",
    "    - r1/r2     Reward predicted for each frame\n",
    "    - rs1/rs2   Reward summed over all frames for each trajectory\n",
    "    - pred      Predicted preference\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, core_network, dropout, batchnorm, lr):\n",
    "        training = tf.placeholder(tf.bool)\n",
    "        # Each element of the batch is one trajectory segment.\n",
    "        # (Dimensions are n segments x n frames per segment x ...)\n",
    "        s1 = tf.placeholder(tf.float32, shape=(None, None, 256, 256))\n",
    "        s2 = tf.placeholder(tf.float32, shape=(None, None, 256, 256))\n",
    "        # For each trajectory segment, there is one human judgement.\n",
    "        pref = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        # Concatenate trajectory segments so that the first dimension is just\n",
    "        # frames\n",
    "        # (necessary because of conv layer's requirements on input shape)\n",
    "        s1_unrolled = tf.reshape(s1, [-1, 256, 256])\n",
    "        s2_unrolled = tf.reshape(s2, [-1, 256, 256])\n",
    "\n",
    "        # Predict rewards for each frame in the unrolled batch\n",
    "        _r1 = core_network(\n",
    "            s=s1_unrolled,\n",
    "            dropout=dropout,\n",
    "            batchnorm=batchnorm,\n",
    "            reuse=False,\n",
    "            training=training)\n",
    "        _r2 = core_network(\n",
    "            s=s2_unrolled,\n",
    "            dropout=dropout,\n",
    "            batchnorm=batchnorm,\n",
    "            reuse=True,\n",
    "            training=training)\n",
    "\n",
    "        # Shape should be 'unrolled batch size'\n",
    "        # where 'unrolled batch size' is 'batch size' x 'n frames per segment'\n",
    "        c1 = tf.assert_rank(_r1, 1)\n",
    "        c2 = tf.assert_rank(_r2, 1)\n",
    "        with tf.control_dependencies([c1, c2]):\n",
    "            # Re-roll to 'batch size' x 'n frames per segment'\n",
    "            __r1 = tf.reshape(_r1, tf.shape(s1)[0:2])\n",
    "            __r2 = tf.reshape(_r2, tf.shape(s2)[0:2])\n",
    "        # Shape should be 'batch size' x 'n frames per segment'\n",
    "        c1 = tf.assert_rank(__r1, 2)\n",
    "        c2 = tf.assert_rank(__r2, 2)\n",
    "        with tf.control_dependencies([c1, c2]):\n",
    "            r1 = __r1\n",
    "            r2 = __r2\n",
    "\n",
    "        # Sum rewards over all frames in each segment\n",
    "        _rs1 = tf.reduce_sum(r1, axis=1)\n",
    "        _rs2 = tf.reduce_sum(r2, axis=1)\n",
    "        # Shape should be 'batch size'\n",
    "        c1 = tf.assert_rank(_rs1, 1)\n",
    "        c2 = tf.assert_rank(_rs2, 1)\n",
    "        with tf.control_dependencies([c1, c2]):\n",
    "            rs1 = _rs1\n",
    "            rs2 = _rs2\n",
    "\n",
    "        # Predict preferences for each segment\n",
    "        _rs = tf.stack([rs1, rs2], axis=1)\n",
    "        # Shape should be 'batch size' x 2\n",
    "        c1 = tf.assert_rank(_rs, 2)\n",
    "        with tf.control_dependencies([c1]):\n",
    "            rs = _rs\n",
    "        _pred = tf.nn.softmax(rs)\n",
    "        # Shape should be 'batch_size' x 2\n",
    "        c1 = tf.assert_rank(_pred, 2)\n",
    "        with tf.control_dependencies([c1]):\n",
    "            pred = _pred\n",
    "\n",
    "        preds_correct = tf.equal(tf.argmax(pref, 1), tf.argmax(pred, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(preds_correct, tf.float32))\n",
    "\n",
    "        _loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=pref,\n",
    "                                                           logits=rs)\n",
    "        # Shape should be 'batch size'\n",
    "        c1 = tf.assert_rank(_loss, 1)\n",
    "        with tf.control_dependencies([c1]):\n",
    "            loss = tf.reduce_sum(_loss)\n",
    "\n",
    "        # Make sure that batch normalization ops are updated\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "        # Inputs\n",
    "        self.training = training\n",
    "        self.s1 = s1\n",
    "        self.s2 = s2\n",
    "        self.pref = pref\n",
    "\n",
    "        # Outputs\n",
    "        self.r1 = r1\n",
    "        self.r2 = r2\n",
    "        self.rs1 = rs1\n",
    "        self.rs2 = rs2\n",
    "        self.pred = pred\n",
    "\n",
    "        self.accuracy = accuracy\n",
    "        self.loss = loss\n",
    "        self.train = train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
