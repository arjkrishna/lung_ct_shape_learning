{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os.path as osp\n",
    "import queue\n",
    "import time\n",
    "\n",
    "import cloudpickle\n",
    "import easy_tf_log\n",
    "import numpy as np\n",
    "from numpy.testing import assert_equal\n",
    "import tensorflow as tf\n",
    "\n",
    "# from a2c import logger\n",
    "from a2c.a2c.utils import (cat_entropy, discount_with_dones,\n",
    "                           find_trainable_variables, mse, vectors_to_images)\n",
    "# from a2c.common import explained_variance, set_global_seeds\n",
    "from pref_db import Segment\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self,\n",
    "                 policy,\n",
    "                 ob_space,\n",
    "                 ac_space,\n",
    "                 nenvs, # 1\n",
    "                 nsteps, # 5 / 9 ?\n",
    "#                  nstack,  \n",
    "#                  num_procs,\n",
    "                 lr_scheduler,\n",
    "                 ent_coef=0.01,\n",
    "                 vf_coef=0.5,\n",
    "                 max_grad_norm=0.5,\n",
    "                 alpha=0.99,\n",
    "                 epsilon=1e-5):\n",
    "#         config = tf.ConfigProto(\n",
    "#             allow_soft_placement=True,\n",
    "#             intra_op_parallelism_threads=num_procs,\n",
    "#             inter_op_parallelism_threads=num_procs)\n",
    "#         config.gpu_options.allow_growth = True\n",
    "#         sess = tf.Session(config=config)\n",
    "        nbatch = nenvs * nsteps   # = 5\n",
    "\n",
    "        A = tf.placeholder(tf.int32, [nbatch])\n",
    "        ADV = tf.placeholder(tf.float32, [nbatch])\n",
    "        R = tf.placeholder(tf.float32, [nbatch])\n",
    "        LR = tf.placeholder(tf.float32, [])\n",
    "\n",
    "        step_model = policy(\n",
    "            sess, ob_space, ac_space, nenvs, 1, reuse=False)\n",
    "        train_model = policy(\n",
    "            sess, ob_space, ac_space, nenvs, nsteps, reuse=True)\n",
    "\n",
    "        neglogpac = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=train_model.pi, labels=A) # pi (nenvs, action_space.n) - action logits of policy ; not returned by policy\n",
    "        pg_loss = tf.reduce_mean(ADV * neglogpac)\n",
    "        vf_loss = tf.reduce_mean(mse(tf.squeeze(train_model.vf), R)) #vf = value ; not returned\n",
    "        entropy = tf.reduce_mean(cat_entropy(train_model.pi))\n",
    "        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef\n",
    "\n",
    "        params = find_trainable_variables(\"model\")\n",
    "        grads = tf.gradients(loss, params)\n",
    "        if max_grad_norm is not None:\n",
    "            grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "        grads = list(zip(grads, params))\n",
    "        trainer = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=LR, decay=alpha, epsilon=epsilon)\n",
    "        _train = trainer.apply_gradients(grads)\n",
    "\n",
    "        def train(obs, rewards, actions, values):\n",
    "            advs = rewards - values\n",
    "            n_steps = len(obs)\n",
    "            for _ in range(n_steps):\n",
    "                cur_lr = lr_scheduler.value()\n",
    "            td_map = {\n",
    "                train_model.X: obs,\n",
    "                A: actions,\n",
    "                ADV: advs,\n",
    "                R: rewards,\n",
    "                LR: cur_lr\n",
    "            }\n",
    "#             if states != []:\n",
    "#                 td_map[train_model.S] = states\n",
    "# #                 td_map[train_model.M] = masks\n",
    "            policy_loss, value_loss, policy_entropy, _ = sess.run(\n",
    "                [pg_loss, vf_loss, entropy, _train], td_map)\n",
    "            return policy_loss, value_loss, policy_entropy, cur_lr\n",
    "\n",
    "        self.train = train\n",
    "        self.train_model = train_model\n",
    "        self.step_model = step_model\n",
    "        self.step = step_model.step\n",
    "        self.value = step_model.value\n",
    "#         self.initial_state = step_model.initial_state\n",
    "        self.sess = sess\n",
    "        # Why var_list=params?\n",
    "        # Otherwise we'll also save optimizer parameters,\n",
    "        # which take up a /lot/ of space.\n",
    "        # Why save_relative_paths=True?\n",
    "        # So that the plain-text 'checkpoint' file written uses relative paths,\n",
    "        # which seems to be needed in order to avoid confusing saver.restore()\n",
    "        # when restoring from FloydHub runs.\n",
    "        self.saver = tf.train.Saver(\n",
    "            max_to_keep=1, var_list=params, save_relative_paths=True)\n",
    "        tf.global_variables_initializer().run(session=sess)\n",
    "\n",
    "    def load(self, ckpt_path):\n",
    "        self.saver.restore(self.sess, ckpt_path)\n",
    "\n",
    "    def save(self, ckpt_path, step_n):\n",
    "        saved_path = self.saver.save(self.sess, ckpt_path, step_n)\n",
    "        print(\"Saved policy checkpoint to '{}'\".format(saved_path))\n",
    "\n",
    "\n",
    "class Runner(object):  # Trains workers, does actions etc and pushed clips for prefs to frontend\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 model,\n",
    "                 nsteps,\n",
    "#                  nstack,\n",
    "#                  gamma,\n",
    "                 gen_segments,\n",
    "                 seg_pipe,\n",
    "                 reward_predictor#,\n",
    "#                  episode_vid_queue\n",
    "                ):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "#         nh, nw, nc = env.observation_space.shape\n",
    "        nh = env.observation_space.shape[0]\n",
    "        nenv = env.num_envs # 8 \n",
    "#         self.batch_ob_shape = (nenv * nsteps, nh, nw, nc * nstack) nstack  = 1, \n",
    "#         since we are not looking back 4 frames\n",
    "        self.batch_ob_shape = (nenv * nsteps, nh)\n",
    "#         self.obs = np.zeros((nenv, nh), dtype=np.uint8)\n",
    "#         # The first stack of 4 frames: the first 3 frames are zeros,\n",
    "#         # with the last frame coming from env.reset().\n",
    "        self.obs, self.states = env.reset()\n",
    "#         self.update_obs(obs)\n",
    "#         self.gamma = gamma\n",
    "        self.nsteps = nsteps\n",
    "#         self.states = model.initial_state\n",
    "        self.dones = [False for _ in range(nenv)]  # [0]\n",
    "\n",
    "        self.gen_segments = gen_segments\n",
    "        self.segment = Segment()\n",
    "        self.seg_pipe = seg_pipe\n",
    "\n",
    "        self.orig_reward = [0 for _ in range(nenv)]  # [0]\n",
    "        self.reward_predictor = reward_predictor\n",
    "\n",
    "#         self.episode_frames = []\n",
    "#         self.episode_vid_queue = episode_vid_queue\n",
    "\n",
    "    def update_segment_buffer(self, mb_states, mb_rewards, mb_dones):\n",
    "        # Segments are only generated from the first worker.\n",
    "        # Empirically, this seems to work fine.\n",
    "        for e0_states, e0_rew, e0_dones in zip(mb_states, mb_rewards, mb_dones):\n",
    "#             e0_obs = mb_obs[0]\n",
    "#             e0_rew = mb_rewards[0]\n",
    "#             e0_dones = mb_dones[0]\n",
    "#             assert_equal(e0_obs.shape, (self.nsteps, 91))  \n",
    "            assert_equal(e0_states.shape, (self.nsteps, 121)) # 50 + 50 + 1 + 20  \n",
    "            assert_equal(e0_rew.shape, (self.nsteps, ))\n",
    "            assert_equal(e0_dones.shape, (self.nsteps, ))\n",
    "\n",
    "            for step in range(self.nsteps):\n",
    "                self.segment.append(np.copy(e0_states[step]), np.copy(e0_rew[step]))\n",
    "                if len(self.segment) == 25 or e0_dones[step]:\n",
    "                    while len(self.segment) < 25:  # maybe 9 max?\n",
    "                        # Pad to 25 steps long so that all segments in the batch\n",
    "                        # have the same length.\n",
    "                        # Note that the reward predictor needs the full frame\n",
    "                        # stack, so we send all frames.\n",
    "                        self.segment.append(e0_states[step], 0)\n",
    "                    self.segment.finalise()\n",
    "                    try:\n",
    "                        self.seg_pipe.put(self.segment, block=False)  \n",
    "                    except queue.Full:\n",
    "                        ### ??? we should wait for half a second or break from the \n",
    "                        ### entire eight segments\n",
    "                        # If the preference interface has a backlog of segments\n",
    "                        # to deal with, don't stop training the agents. Just drop\n",
    "                        # the segment and keep on going.\n",
    "                        pass\n",
    "                    self.segment = Segment()\n",
    "\n",
    "#     def update_episode_frame_buffer(self, mb_obs, mb_dones):\n",
    "#         e0_obs = mb_obs[0]\n",
    "#         e0_dones = mb_dones[0]\n",
    "#         for step in range(self.nsteps):\n",
    "#             # Here we only need to send the last frame (the most recent one)\n",
    "#             # from the 4-frame stack, because we're just showing output to\n",
    "#             # the user.\n",
    "#             self.episode_frames.append(e0_obs[step, :, :, -1])\n",
    "#             if e0_dones[step]:\n",
    "#                 self.episode_vid_queue.put(self.episode_frames)\n",
    "#                 self.episode_frames = []\n",
    "\n",
    "    def run(self):\n",
    "        nenvs = len(self.env.remotes)\n",
    "        mb_obs, mb_states, mb_rewards, mb_actions, mb_values, mb_dones = \\\n",
    "            [], [], [], [], [], []\n",
    "#         mb_states = self.states\n",
    "\n",
    "        # Run for nsteps steps in the environment\n",
    "        for _ in range(self.nsteps):\n",
    "            actions, values, _ = self.model.step(self.obs)\n",
    "            mb_obs.append(np.copy(self.obs))\n",
    "            mb_states.append(np.copy(self.states))\n",
    "            mb_actions.append(actions)\n",
    "            mb_values.append(values)\n",
    "            mb_dones.append(self.dones)\n",
    "            # len({obs, rewards, dones}) == nenvs\n",
    "            obs, rewards, dones, states = self.env.step(actions) # remove rewards\n",
    "#             self.states = states\n",
    "            self.dones = dones\n",
    "#             for n, done in enumerate(dones):\n",
    "#                 if done:\n",
    "#                     self.obs[n] = self.obs[n] * 0\n",
    "            # SubprocVecEnv automatically resets when done  ??? -- Resolved\n",
    "            self.obs = obs\n",
    "            self.states = states\n",
    "            mb_rewards.append(rewards)\n",
    "        mb_dones.append(self.dones)\n",
    "        # batch of steps to batch of rollouts\n",
    "        # i.e. from nsteps, nenvs to nenvs, nsteps\n",
    "        mb_obs = np.asarray(mb_obs, dtype=np.uint8).swapaxes(1, 0)\n",
    "        mb_states = np.asarray(mb_states, dtype=np.uint8).swapaxes(1, 0)\n",
    "        mb_rewards = np.asarray(mb_rewards, dtype=np.float32).swapaxes(1, 0)\n",
    "        mb_actions = np.asarray(mb_actions, dtype=np.int32).swapaxes(1, 0)\n",
    "        mb_values = np.asarray(mb_values, dtype=np.float32).swapaxes(1, 0)\n",
    "        mb_dones = np.asarray(mb_dones, dtype=np.bool).swapaxes(1, 0)\n",
    "#         mb_masks = mb_dones[:, :-1]\n",
    "        # The first entry was just the init state of 'dones' (all False),\n",
    "        # before we'd actually run any steps, so drop it.\n",
    "        mb_dones = mb_dones[:, 1:]\n",
    "\n",
    "        # Log original rewards\n",
    "#         for env_n, (rs, dones) in enumerate(zip(mb_rewards, mb_dones)):\n",
    "#             assert_equal(rs.shape, (self.nsteps, ))\n",
    "#             assert_equal(dones.shape, (self.nsteps, ))\n",
    "#             for step_n in range(self.nsteps):\n",
    "#                 self.orig_reward[env_n] += rs[step_n]\n",
    "#                 if dones[step_n]:\n",
    "#                     easy_tf_log.tflog(\n",
    "#                         \"orig_reward_{}\".format(env_n),\n",
    "#                         self.orig_reward[env_n])\n",
    "#                     self.orig_reward[env_n] = 0\n",
    "\n",
    "\n",
    "        # Generate segments\n",
    "        # (For MovingDot, this has to happen _after_ we've encoded the action\n",
    "        # in the observations.)\n",
    "        if self.gen_segments: # should be after reward_predictor ??? same concept found in\n",
    "            #pref interface\n",
    "            # run it for only one environment, get eight different outputs\n",
    "            # and \n",
    "#             for _ in range(self.nsteps):\n",
    "#             obs = mb_obs[0]\n",
    "#             action, alt_action, _, _ = self.model.step(obs)\n",
    "#             action_act, alt_action_act, _, _ = self.model.step(obs)\n",
    "#             action, alt_action, _, _ = self.model.step(self.obs)\n",
    "        \n",
    "#             gen1_mb_obs.append(np.copy(self.obs))\n",
    "#             gen2_mb_obs.append(np.copy(self.obs))\n",
    "#             gen3_mb_obs.append(np.copy(self.obs))\n",
    "#             gen4_mb_obs.append(np.copy(self.obs))\n",
    "#             gen5_mb_obs.append(np.copy(self.obs))\n",
    "#             gen6_mb_obs.append(np.copy(self.obs))\n",
    "#             gen7_mb_obs.append(np.copy(self.obs))\n",
    "#             gen8_mb_obs.append(np.copy(self.obs))\n",
    "#             mb_dones.append(self.dones)\n",
    "#             # len({obs, rewards, dones}) == nenvs\n",
    "#             obs, rewards, dones, _ = self.env.step(actions) # remove rewards\n",
    "#             self.dones = dones\n",
    "#             self.obs = obs\n",
    "#             mb_rewards.append(rewards)\n",
    "            self.update_segment_buffer(mb_states, mb_rewards, mb_dones)\n",
    "\n",
    "        # Replace rewards with those from reward predictor\n",
    "        # (Note that this also needs to be done _after_ we've encoded the\n",
    "        # action.)\n",
    "        logging.debug(\"Original rewards:\\n%s\", mb_rewards)\n",
    "        if self.reward_predictor: # Always true in our case # Find a way to merge both rewards\n",
    "            assert_equal(mb_states.shape, (nenvs, self.nsteps, 121)) # this is where the shap.....\n",
    "            mb_states_allenvs = mb_states.reshape(nenvs * self.nsteps, 121)\n",
    "            mb_images_allenvs, rew = vectors_to_images(mb_states_allenvs)\n",
    "            if mb_images_allenvs == None:\n",
    "                rewards_allenvs = np.zeros((nenvs * self.nsteps,))\n",
    "            else:\n",
    "                assert_equal(mb_images_allenvs, (nenvs * self.nsteps, 512, 512))\n",
    "                rewards_allenvs = self.reward_predictor.reward(mb_images_allenvs)\n",
    "                rewards_allenvs = np.where(rew == 0, -1, rewards_allenvs)\n",
    "                assert_equal(rewards_allenvs.shape, (nenvs * self.nsteps, ))\n",
    "            ### TODO : make -1/-2.. and 1 # Can also use np.where --- DONE\n",
    "#             mb_rewards = np.multiply(mb_rewards*(rewards_allenvs.reshape(nenvs, self.nsteps)))\n",
    "            rewards_allenvs = rewards_allenvs.reshape(nenvs, self.nsteps)\n",
    "            mb_rewards = np.where(mb_rewards==-1, mb_rewards, rewards_allenvs)\n",
    "            assert_equal(mb_rewards.shape, (nenvs, self.nsteps))\n",
    "\n",
    "            logging.debug(\"Predicted rewards:\\n%s\", mb_rewards)\n",
    "\n",
    "        # Save frames for episode rendering\n",
    "#         if self.episode_vid_queue is not None:\n",
    "#             self.update_episode_frame_buffer(mb_obs, mb_dones)\n",
    "\n",
    "        # Discount rewards\n",
    "        mb_obs = mb_obs.reshape(self.batch_ob_shape)\n",
    "        last_values = self.model.value(self.obs).tolist()\n",
    "        # discount/bootstrap off value fn\n",
    "        for n, (rewards, dones, value) in enumerate(\n",
    "                zip(mb_rewards, mb_dones, last_values)):\n",
    "            rewards = rewards.tolist()\n",
    "            dones = dones.tolist()\n",
    "            if dones[-1] == 0:\n",
    "                # Make sure that the first iteration of the loop inside\n",
    "                # discount_with_dones picks up 'value' as the initial       #???\n",
    "                # value of r\n",
    "                rewards = discount_with_dones(rewards + [value],\n",
    "                                              dones + [0],\n",
    "                                              self.gamma)[:-1]\n",
    "            else:\n",
    "                rewards = discount_with_dones(rewards, dones, self.gamma)\n",
    "            mb_rewards[n] = rewards\n",
    "\n",
    "        mb_rewards = mb_rewards.flatten()\n",
    "        mb_actions = mb_actions.flatten()\n",
    "        mb_values = mb_values.flatten()\n",
    "#         mb_masks = mb_masks.flatten()\n",
    "        return mb_obs, mb_rewards, mb_actions, mb_values\n",
    "\n",
    "\n",
    "def learn(policy,\n",
    "          env,\n",
    "#           seed,  ### ??? -- resolved\n",
    "          start_policy_training_pipe,  \n",
    "          ckpt_save_dir,\n",
    "          lr_scheduler,\n",
    "          nsteps=5,\n",
    "#           nstack=4,\n",
    "          total_timesteps=int(80e6),\n",
    "          vf_coef=0.5,\n",
    "          ent_coef=0.01,\n",
    "          max_grad_norm=0.5,\n",
    "          epsilon=1e-5,\n",
    "          alpha=0.99,\n",
    "          gamma=0.99,\n",
    "          log_interval=100,\n",
    "          ckpt_save_interval=1000,\n",
    "          ckpt_load_dir=None,\n",
    "          gen_segments=False,\n",
    "          seg_pipe=None,\n",
    "          reward_predictor=None,\n",
    "#           episode_vid_queue=None):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "#     set_global_seeds(seed)\n",
    "\n",
    "    nenvs = env.num_envs\n",
    "    ob_space = env.observation_space\n",
    "    ac_space = env.action_space\n",
    "    num_procs = len(env.remotes)  # HACK\n",
    "\n",
    "    def make_model():\n",
    "        return Model(\n",
    "            policy=policy,\n",
    "            ob_space=ob_space,\n",
    "            ac_space=ac_space,\n",
    "            nenvs=nenvs,\n",
    "            nsteps=nsteps,\n",
    "#             nstack=nstack,\n",
    "            num_procs=num_procs,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "            alpha=alpha,\n",
    "            epsilon=epsilon)\n",
    "\n",
    "#     with open(osp.join(ckpt_save_dir, 'make_model.pkl'), 'wb') as fh:\n",
    "#         fh.write(cloudpickle.dumps(make_model))\n",
    "\n",
    "    print(\"Initialising policy...\")\n",
    "    if ckpt_load_dir is None:\n",
    "        model = make_model()\n",
    "    else:\n",
    "        with open(osp.join(ckpt_load_dir, 'make_model.pkl'), 'rb') as fh:\n",
    "            make_model = cloudpickle.loads(fh.read())\n",
    "        model = make_model()\n",
    "\n",
    "        ckpt_load_path = tf.train.latest_checkpoint(ckpt_load_dir)\n",
    "        model.load(ckpt_load_path)\n",
    "        print(\"Loaded policy from checkpoint '{}'\".format(ckpt_load_path))\n",
    "\n",
    "    ckpt_save_path = osp.join(ckpt_save_dir, 'policy.ckpt')\n",
    "\n",
    "    runner = Runner(env=env,\n",
    "                    model=model,\n",
    "                    nsteps=nsteps,\n",
    "#                     nstack=nstack,\n",
    "                    gamma=gamma,\n",
    "                    gen_segments=gen_segments,\n",
    "                    seg_pipe=seg_pipe,\n",
    "                    reward_predictor=reward_predictor#,\n",
    "#                     episode_vid_queue=episode_vid_queue\n",
    "                   )\n",
    "\n",
    "    # nsteps: e.g. 5\n",
    "    # nenvs: e.g. 16\n",
    "    nbatch = nenvs * nsteps\n",
    "    fps_tstart = time.time()\n",
    "    fps_nsteps = 0\n",
    "\n",
    "    print(\"Starting workers\")\n",
    "\n",
    "    # Before we're told to start training the policy itself,\n",
    "    # just generate segments for the reward predictor to be trained with    #not needed since pretrained\n",
    "    while True:\n",
    "        runner.run()\n",
    "        try:\n",
    "            start_policy_training_pipe.get(block=False)  ### ???\n",
    "        except queue.Empty:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"Starting policy training\")\n",
    "\n",
    "    for update in range(1, total_timesteps // nbatch + 1):\n",
    "        # Run for nsteps\n",
    "        obs, states, rewards, actions, values = runner.run()\n",
    "\n",
    "        policy_loss, value_loss, policy_entropy, cur_lr = model.train(\n",
    "            obs, states, rewards, actions, values)\n",
    "\n",
    "        fps_nsteps += nbatch\n",
    "\n",
    "        if update % log_interval == 0 and update != 0:\n",
    "            fps = fps_nsteps / (time.time() - fps_tstart)\n",
    "            fps_nsteps = 0\n",
    "            fps_tstart = time.time()\n",
    "\n",
    "            print(\"Trained policy for {} time steps\".format(update * nbatch))\n",
    "\n",
    "            ev = explained_variance(values, rewards)  ### ???\n",
    "            logger.record_tabular(\"nupdates\", update)\n",
    "            logger.record_tabular(\"total_timesteps\", update * nbatch)\n",
    "            logger.record_tabular(\"fps\", fps)\n",
    "            logger.record_tabular(\"policy_entropy\", float(policy_entropy))\n",
    "            logger.record_tabular(\"value_loss\", float(value_loss))\n",
    "            logger.record_tabular(\"explained_variance\", float(ev))\n",
    "            logger.record_tabular(\"learning_rate\", cur_lr)\n",
    "            logger.dump_tabular()\n",
    "\n",
    "        if update != 0 and update % ckpt_save_interval == 0:\n",
    "            model.save(ckpt_save_path, update)\n",
    "\n",
    "    model.save(ckpt_save_path, update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
