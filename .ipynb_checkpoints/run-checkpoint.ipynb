{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from os import path as osp\n",
    "import sys\n",
    "import time\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "import cloudpickle\n",
    "import easy_tf_log\n",
    "from a2c import logger\n",
    "from a2c.a2c.a2c import learn\n",
    "from a2c.a2c.policies import CnnPolicy, MlpPolicy\n",
    "from ct_env_n import CustomEnv\n",
    "# from a2c.common import set_global_seeds\n",
    "# from a2c.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "# from params import parse_args, PREFS_VAL_FRACTION\n",
    "from pref_db import PrefDB, PrefBuffer\n",
    "from pref_interface import PrefInterface\n",
    "from reward_predictor import RewardPredictorEnsemble\n",
    "from reward_predictor_core_network import net_cnn, net_moving_dot_features\n",
    "# from utils import VideoRenderer, get_port_range, make_env\n",
    "from utils import get_port_range\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # filter out INFO messages\n",
    "\n",
    "\n",
    "def main():\n",
    "    general_params, a2c_params, \\\n",
    "        pref_interface_params, rew_pred_training_params = parse_args()\n",
    "\n",
    "    if general_params['debug']:\n",
    "        logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "    run(general_params,\n",
    "        a2c_params,\n",
    "        pref_interface_params,\n",
    "        rew_pred_training_params)\n",
    "\n",
    "\n",
    "def run(general_params,\n",
    "        a2c_params,\n",
    "        pref_interface_params,\n",
    "        rew_pred_training_params):\n",
    "    seg_pipe = Queue(maxsize=1)\n",
    "    pref_pipe = Queue(maxsize=1)\n",
    "    start_policy_training_flag = Queue(maxsize=1)  #???\n",
    "\n",
    "    reward_predictor_network #= net_cnn\n",
    "\n",
    "    def make_reward_predictor(name, cluster_dict):\n",
    "        return RewardPredictorEnsemble(\n",
    "            cluster_job_name=name,\n",
    "            cluster_dict=cluster_dict,\n",
    "            log_dir=general_params['log_dir'],\n",
    "#             batchnorm=rew_pred_training_params['batchnorm'],\n",
    "#             dropout=rew_pred_training_params['dropout'],\n",
    "            lr=rew_pred_training_params['lr'],\n",
    "            core_network=reward_predictor_network)\n",
    "\n",
    "    save_make_reward_predictor(general_params['log_dir'],\n",
    "                               make_reward_predictor)\n",
    "    \n",
    "    if general_params['mode'] == 'gather_initial_prefs':\n",
    "        cluster_dict = create_cluster_dict(['a2c'])   ### ??? ERROR\n",
    "#         ps_proc = start_parameter_server(cluster_dict, make_reward_predictor)  # this needs to be commented out\n",
    "        env, a2c_proc = start_policy_training(\n",
    "            cluster_dict=cluster_dict,\n",
    "            make_reward_predictor=make_reward_predictor,  # since reward pred is used to get rewards\n",
    "            gen_segments=True,\n",
    "            start_policy_training_pipe=start_policy_training_flag,\n",
    "            seg_pipe=seg_pipe,\n",
    "#             episode_vid_queue=episode_vid_queue,\n",
    "            log_dir=general_params['log_dir'],\n",
    "            a2c_params=a2c_params)\n",
    "        pi, pi_proc = start_pref_interface(   # gather preferences through interface\n",
    "            seg_pipe=seg_pipe,\n",
    "            pref_pipe=pref_pipe,\n",
    "            log_dir=general_params['log_dir'],\n",
    "            **pref_interface_params)\n",
    "\n",
    "        n_train = general_params['max_prefs'] * (1 - PREFS_VAL_FRACTION)\n",
    "        n_val = general_params['max_prefs'] * PREFS_VAL_FRACTION\n",
    "        pref_db_train = PrefDB(maxlen=n_train)\n",
    "        pref_db_val = PrefDB(maxlen=n_val)\n",
    "        pref_buffer = PrefBuffer(db_train=pref_db_train, db_val=pref_db_val)\n",
    "        pref_buffer.start_recv_thread(pref_pipe)\n",
    "        pref_buffer.wait_until_len(general_params['n_initial_prefs'])\n",
    "        pref_db_train, pref_db_val = pref_buffer.get_dbs()\n",
    "\n",
    "        save_prefs(general_params['log_dir'], pref_db_train, pref_db_val)\n",
    "\n",
    "        pi_proc.terminate()\n",
    "        pi.stop_renderer()\n",
    "        a2c_proc.terminate()\n",
    "        pref_buffer.stop_recv_thread()\n",
    "\n",
    "        env.close()\n",
    "    elif general_params['mode'] == 'just_train_reward_predictor':     #no front end\n",
    "        cluster_dict = create_cluster_dict(['ps', 'train'])\n",
    "        ps_proc = start_parameter_server(cluster_dict, make_reward_predictor)\n",
    "        rpt_proc = start_reward_predictor_training(\n",
    "            cluster_dict=cluster_dict,\n",
    "            make_reward_predictor=make_reward_predictor,\n",
    "            just_pretrain=True,\n",
    "            pref_pipe=pref_pipe,\n",
    "            start_policy_training_pipe=start_policy_training_flag,\n",
    "            max_prefs=general_params['max_prefs'],\n",
    "            prefs_dir=general_params['prefs_dir'],\n",
    "            load_ckpt_dir=None,\n",
    "            n_initial_prefs=general_params['n_initial_prefs'],\n",
    "            n_initial_epochs=rew_pred_training_params['n_initial_epochs'],\n",
    "            val_interval=rew_pred_training_params['val_interval'],\n",
    "            ckpt_interval=rew_pred_training_params['ckpt_interval'],\n",
    "            log_dir=general_params['log_dir'])\n",
    "        rpt_proc.join()\n",
    "        ps_proc.terminate()\n",
    "    elif general_params['mode'] == 'train_policy_with_preferences':  #everything\n",
    "        cluster_dict = create_cluster_dict(['ps', 'a2c', 'train'])\n",
    "        ps_proc = start_parameter_server(cluster_dict, make_reward_predictor)\n",
    "        env, a2c_proc = start_policy_training(\n",
    "            cluster_dict=cluster_dict,\n",
    "            make_reward_predictor=make_reward_predictor,\n",
    "            gen_segments=True,\n",
    "            start_policy_training_pipe=start_policy_training_flag,\n",
    "            seg_pipe=seg_pipe,\n",
    "            episode_vid_queue=episode_vid_queue,\n",
    "            log_dir=general_params['log_dir'],\n",
    "            a2c_params=a2c_params)\n",
    "        pi, pi_proc = start_pref_interface(\n",
    "            seg_pipe=seg_pipe,\n",
    "            pref_pipe=pref_pipe,\n",
    "            log_dir=general_params['log_dir'],\n",
    "            **pref_interface_params)\n",
    "        rpt_proc = start_reward_predictor_training(\n",
    "            cluster_dict=cluster_dict,\n",
    "            make_reward_predictor=make_reward_predictor,\n",
    "            just_pretrain=False,\n",
    "            pref_pipe=pref_pipe,\n",
    "            start_policy_training_pipe=start_policy_training_flag,\n",
    "            max_prefs=general_params['max_prefs'],\n",
    "            prefs_dir=general_params['prefs_dir'],\n",
    "            load_ckpt_dir=rew_pred_training_params['load_ckpt_dir'],\n",
    "            n_initial_prefs=general_params['n_initial_prefs'],\n",
    "            n_initial_epochs=rew_pred_training_params['n_initial_epochs'],\n",
    "            val_interval=rew_pred_training_params['val_interval'],\n",
    "            ckpt_interval=rew_pred_training_params['ckpt_interval'],\n",
    "            log_dir=general_params['log_dir'])\n",
    "        # We wait for A2C to complete the specified number of policy training\n",
    "        # steps\n",
    "        a2c_proc.join()\n",
    "        rpt_proc.terminate()\n",
    "        pi_proc.terminate()\n",
    "        pi.stop_renderer()\n",
    "        ps_proc.terminate()\n",
    "        env.close()\n",
    "    else:\n",
    "        raise Exception(\"Unknown mode: {}\".format(general_params['mode']))\n",
    "\n",
    "\n",
    "def save_prefs(log_dir, pref_db_train, pref_db_val):\n",
    "    train_path = osp.join(log_dir, 'train.pkl.gz')\n",
    "    pref_db_train.save(train_path)\n",
    "    print(\"Saved training preferences to '{}'\".format(train_path))\n",
    "    val_path = osp.join(log_dir, 'val.pkl.gz')\n",
    "    pref_db_val.save(val_path)\n",
    "    print(\"Saved validation preferences to '{}'\".format(val_path))\n",
    "\n",
    "\n",
    "def save_make_reward_predictor(log_dir, make_reward_predictor):\n",
    "    save_dir = osp.join(log_dir, 'reward_predictor_checkpoints')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(osp.join(save_dir, 'make_reward_predictor.pkl'), 'wb') as fh:\n",
    "        fh.write(cloudpickle.dumps(make_reward_predictor))\n",
    "\n",
    "\n",
    "def create_cluster_dict(jobs):\n",
    "    n_ports = len(jobs) + 1\n",
    "    ports = get_port_range(start_port=2200,\n",
    "                           n_ports=n_ports,\n",
    "                           random_stagger=True)\n",
    "    cluster_dict = {}\n",
    "    for part, port in zip(jobs, ports):\n",
    "        cluster_dict[part] = ['localhost:{}'.format(port)]\n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "def configure_a2c_logger(log_dir):\n",
    "    a2c_dir = osp.join(log_dir, 'a2c')\n",
    "    os.makedirs(a2c_dir)\n",
    "    tb = logger.TensorBoardOutputFormat(a2c_dir)\n",
    "    logger.Logger.CURRENT = logger.Logger(dir=a2c_dir, output_formats=[tb])\n",
    "\n",
    "\n",
    "# def make_envs(n_envs):\n",
    "# #     def wrap_make_env(env_id, rank):\n",
    "# #         def _thunk():\n",
    "# #             return make_env(env_id, seed + rank)\n",
    "# #         return _thunk\n",
    "# # #     set_global_seeds(seed)\n",
    "# #     env = SubprocVecEnv(env_id, [wrap_make_env(env_id, i)\n",
    "# #                                  for i in range(n_envs)])\n",
    "#     env = CustomEnv(n_envs)\n",
    "#     return env\n",
    "\n",
    "\n",
    "def start_parameter_server(cluster_dict, make_reward_predictor):\n",
    "    def f():\n",
    "        make_reward_predictor('ps', cluster_dict)\n",
    "        while True:\n",
    "            time.sleep(1.0)\n",
    "\n",
    "    proc = Process(target=f, daemon=True)\n",
    "    proc.start()\n",
    "    return proc\n",
    "\n",
    "# Train Button -- \"In interface\"\n",
    "# we have to start listening for running this main program as well\n",
    "def start_policy_training(cluster_dict, make_reward_predictor, gen_segments,\n",
    "                          start_policy_training_pipe, seg_pipe,\n",
    "                          episode_vid_queue, log_dir, a2c_params):\n",
    "    policy_fn = MlpPolicy\n",
    "\n",
    "#     configure_a2c_logger(log_dir)\n",
    "\n",
    "    # Done here because daemonic processes can't have children\n",
    "    env = CustomEnv(#a2c_params['env_id'],\n",
    "                    a2c_params['n_envs'])\n",
    "                    # a2c_params['seed']) # seed ??? -solved\n",
    "    del a2c_params['env_id'], a2c_params['n_envs']\n",
    "\n",
    "    ckpt_dir = osp.join(log_dir, 'policy_checkpoints')\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "    def f():\n",
    "        reward_predictor ## get  = make_reward_predictor('a2c', cluster_dict) #load_rewa...\n",
    "        misc_logs_dir = osp.join(log_dir, 'a2c_misc')\n",
    "        easy_tf_log.set_dir(misc_logs_dir)\n",
    "        learn(\n",
    "            policy=policy_fn,\n",
    "            env=env,\n",
    "            seg_pipe=seg_pipe,\n",
    "            start_policy_training_pipe=start_policy_training_pipe, #???\n",
    "#             episode_vid_queue=episode_vid_queue, # no need since no video \n",
    "            reward_predictor=reward_predictor,\n",
    "            ckpt_save_dir=ckpt_dir,\n",
    "            gen_segments=gen_segments,\n",
    "            **a2c_params)\n",
    "\n",
    "    proc = Process(target=f, daemon=True)\n",
    "    proc.start()\n",
    "    return env, proc\n",
    "\n",
    "\n",
    "def start_pref_interface(seg_pipe, pref_pipe, max_segs, synthetic_prefs,   #Front-end\n",
    "                         log_dir):\n",
    "    def f():\n",
    "        # The preference interface needs to get input from stdin. stdin is\n",
    "        # automatically closed at the beginning of child processes in Python,\n",
    "        # so this is a bit of a hack, but it seems to be fine.\n",
    "        sys.stdin = os.fdopen(0)\n",
    "        pi.run(seg_pipe=seg_pipe, pref_pipe=pref_pipe)\n",
    "\n",
    "    # Needs to be done in the main process because does GUI setup work\n",
    "    prefs_log_dir = osp.join(log_dir, 'pref_interface')\n",
    "    pi = PrefInterface(synthetic_prefs=synthetic_prefs,\n",
    "                       max_segs=max_segs,\n",
    "                       log_dir=prefs_log_dir)\n",
    "    proc = Process(target=f, daemon=True)\n",
    "    proc.start()\n",
    "    return pi, proc\n",
    "\n",
    "\n",
    "def start_reward_predictor_training(cluster_dict,\n",
    "                                    make_reward_predictor,\n",
    "                                    just_pretrain,\n",
    "                                    pref_pipe,\n",
    "                                    start_policy_training_pipe,\n",
    "                                    max_prefs,\n",
    "                                    n_initial_prefs,\n",
    "                                    n_initial_epochs,\n",
    "                                    prefs_dir,\n",
    "                                    load_ckpt_dir,\n",
    "                                    val_interval,\n",
    "                                    ckpt_interval,\n",
    "                                    log_dir):\n",
    "    def f():\n",
    "        rew_pred = make_reward_predictor('train', cluster_dict)\n",
    "        rew_pred.init_network(load_ckpt_dir)\n",
    "\n",
    "        if prefs_dir is not None:\n",
    "            train_path = osp.join(prefs_dir, 'train.pkl.gz')\n",
    "            pref_db_train = PrefDB.load(train_path)\n",
    "            print(\"Loaded training preferences from '{}'\".format(train_path))\n",
    "            n_prefs, n_segs = len(pref_db_train), len(pref_db_train.segments)  # this could be where we pick pre-\n",
    "            print(\"({} preferences, {} segments)\".format(n_prefs, n_segs)) # reward pred and train more\n",
    "\n",
    "            val_path = osp.join(prefs_dir, 'val.pkl.gz')\n",
    "            pref_db_val = PrefDB.load(val_path)\n",
    "            print(\"Loaded validation preferences from '{}'\".format(val_path))\n",
    "            n_prefs, n_segs = len(pref_db_val), len(pref_db_val.segments)\n",
    "            print(\"({} preferences, {} segments)\".format(n_prefs, n_segs))\n",
    "        else:\n",
    "            n_train = max_prefs * (1 - PREFS_VAL_FRACTION)\n",
    "            n_val = max_prefs * PREFS_VAL_FRACTION\n",
    "            pref_db_train = PrefDB(maxlen=n_train)\n",
    "            pref_db_val = PrefDB(maxlen=n_val)\n",
    "\n",
    "        pref_buffer = PrefBuffer(db_train=pref_db_train,\n",
    "                                 db_val=pref_db_val)\n",
    "        pref_buffer.start_recv_thread(pref_pipe)\n",
    "        if prefs_dir is None:\n",
    "            pref_buffer.wait_until_len(n_initial_prefs)\n",
    "\n",
    "        save_prefs(log_dir, pref_db_train, pref_db_val)\n",
    "\n",
    "        if not load_ckpt_dir:  # just_pretrain = true\n",
    "            print(\"Pretraining reward predictor for {} epochs\".format(\n",
    "                n_initial_epochs))\n",
    "            pref_db_train, pref_db_val = pref_buffer.get_dbs()\n",
    "            for i in range(n_initial_epochs):\n",
    "                # Note that we deliberately don't update the preferences\n",
    "                # databases during pretraining to keep the number of\n",
    "                # fairly preferences small so that pretraining doesn't take too\n",
    "                # long.\n",
    "                print(\"Reward predictor training epoch {}\".format(i))\n",
    "                rew_pred.train(pref_db_train, pref_db_val, val_interval)\n",
    "                if i and i % ckpt_interval == 0:\n",
    "                    rew_pred.save()\n",
    "            print(\"Reward predictor pretraining done\")\n",
    "            rew_pred.save()\n",
    "\n",
    "        if just_pretrain:\n",
    "            return\n",
    "\n",
    "        start_policy_training_pipe.put(True)  #??? maybe to indicate policy is getng trai..\n",
    "        \n",
    "        i = 0\n",
    "        while True:\n",
    "            pref_db_train, pref_db_val = pref_buffer.get_dbs()\n",
    "            save_prefs(log_dir, pref_db_train, pref_db_val)\n",
    "            rew_pred.train(pref_db_train, pref_db_val, val_interval)\n",
    "            if i and i % ckpt_interval == 0:\n",
    "                rew_pred.save()\n",
    "\n",
    "    proc = Process(target=f, daemon=True)\n",
    "    proc.start()\n",
    "    return proc\n",
    "\n",
    "\n",
    "# def start_episode_renderer():               #Front end\n",
    "#     episode_vid_queue = Queue()\n",
    "#     renderer = VideoRenderer(\n",
    "#         episode_vid_queue,\n",
    "#         playback_speed=2,\n",
    "#         zoom=2,\n",
    "#         mode=VideoRenderer.play_through_mode)\n",
    "#     return episode_vid_queue, renderer\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
