{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import random\n",
    "import socket\n",
    "import time\n",
    "import pickle\n",
    "from multiprocessing import Process\n",
    "from scipy.interpolate import splprep, splev\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pyglet\n",
    "\n",
    "from a2c.common.atari_wrappers import wrap_deepmind\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "\n",
    "# https://github.com/joschu/modular_rl/blob/master/modular_rl/running_stat.py\n",
    "# http://www.johndcook.com/blog/standard_deviation/\n",
    "class RunningStat(object):\n",
    "    def __init__(self, shape=()):\n",
    "        self._n = 0\n",
    "        self._M = np.zeros(shape)\n",
    "        self._S = np.zeros(shape)\n",
    "\n",
    "    def push(self, x):\n",
    "        x = np.asarray(x)\n",
    "        assert x.shape == self._M.shape\n",
    "        self._n += 1\n",
    "        if self._n == 1:\n",
    "            self._M[...] = x\n",
    "        else:\n",
    "            oldM = self._M.copy()\n",
    "            self._M[...] = oldM + (x - oldM)/self._n\n",
    "            self._S[...] = self._S + (x - oldM)*(x - self._M)\n",
    "\n",
    "    @property\n",
    "    def n(self):\n",
    "        return self._n\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self._M\n",
    "\n",
    "    @property\n",
    "    def var(self):\n",
    "        if self._n >= 2:\n",
    "            return self._S/(self._n - 1)\n",
    "        else:\n",
    "            return np.square(self._M)\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(self.var)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._M.shape\n",
    "\n",
    "\n",
    "# Based on SimpleImageViewer in OpenAI gym\n",
    "class Im(object):\n",
    "    def __init__(self, display=None):\n",
    "        self.window = None\n",
    "        self.isopen = False\n",
    "        self.display = display\n",
    "\n",
    "    def imshow(self, arr):\n",
    "        if self.window is None:\n",
    "            height, width = arr.shape\n",
    "            self.window = pyglet.window.Window(\n",
    "                width=width, height=height, display=self.display)\n",
    "            self.width = width\n",
    "            self.height = height\n",
    "            self.isopen = True\n",
    "\n",
    "        assert arr.shape == (self.height, self.width), \\\n",
    "            \"You passed in an image with the wrong number shape\"\n",
    "\n",
    "        image = pyglet.image.ImageData(self.width, self.height,\n",
    "                                       'L', arr.tobytes(), pitch=-self.width)\n",
    "        self.window.clear()\n",
    "        self.window.switch_to()\n",
    "        self.window.dispatch_events()\n",
    "        image.blit(0, 0)\n",
    "        self.window.flip()\n",
    "\n",
    "    def close(self):\n",
    "        if self.isopen:\n",
    "            self.window.close()\n",
    "            self.isopen = False\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "def get_port_range(start_port, n_ports, random_stagger=False):\n",
    "    # If multiple runs try and call this function at the same time,\n",
    "    # the function could return the same port range.\n",
    "    # To guard against this, automatically offset the port range.\n",
    "    if random_stagger:\n",
    "        start_port += random.randint(0, 20) * n_ports\n",
    "\n",
    "    free_range_found = False\n",
    "    while not free_range_found:\n",
    "        ports = []\n",
    "        for port_n in range(n_ports):\n",
    "            port = start_port + port_n\n",
    "            try:\n",
    "                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "                s.bind((\"127.0.0.1\", port))\n",
    "                ports.append(port)\n",
    "            except socket.error as e:\n",
    "                if e.errno == 98 or e.errno == 48:\n",
    "                    print(\"Warning: port {} already in use\".format(port))\n",
    "                    break\n",
    "                else:\n",
    "                    raise e\n",
    "            finally:\n",
    "                s.close()\n",
    "        if len(ports) < n_ports:\n",
    "            # The last port we tried was in use\n",
    "            # Try again, starting from the next port\n",
    "            start_port = port + 1\n",
    "        else:\n",
    "            free_range_found = True\n",
    "\n",
    "    return ports\n",
    "\n",
    "\n",
    "def profile_memory(log_path, pid):\n",
    "    import memory_profiler\n",
    "    def profile():\n",
    "        with open(log_path, 'w') as f:\n",
    "            # timeout=99999 is necessary because for external processes,\n",
    "            # memory_usage otherwise defaults to only returning a single sample\n",
    "            # Note that even with interval=1, because memory_profiler only\n",
    "            # flushes every 50 lines, we still have to wait 50 seconds before\n",
    "            # updates.\n",
    "            memory_profiler.memory_usage(pid, stream=f,\n",
    "                                         timeout=99999, interval=1)\n",
    "    p = Process(target=profile, daemon=True)\n",
    "    p.start()\n",
    "    return p\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, shuffle=False):\n",
    "    idxs = list(range(len(data)))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idxs)  # in-place\n",
    "\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    while end_idx < len(data):\n",
    "        end_idx = start_idx + batch_size\n",
    "        if end_idx > len(data):\n",
    "            end_idx = len(data)\n",
    "\n",
    "        batch_idxs = idxs[start_idx:end_idx]\n",
    "        batch = []\n",
    "        for idx in batch_idxs:\n",
    "            batch.append(data[idx])\n",
    "\n",
    "        yield batch\n",
    "        start_idx += batch_size\n",
    "\n",
    "\n",
    "def make_env(env_id, seed=0):\n",
    "    import ct_env\n",
    "    env = gym.make(env_id)\n",
    "    env.seed(seed)\n",
    "    return wrap_deepmind(env)\n",
    "\n",
    "def get_first_grp_struct():\n",
    "    ll = []\n",
    "    for x in range(12):\n",
    "        ll.append([x, x*2 + 12, x*2 + 13, x+1])\n",
    "    ll[-1][-1] = 0\n",
    "    co_in = []\n",
    "    for i,z in enumerate(ll):\n",
    "        if i == 0:\n",
    "            co_in.extend(z)\n",
    "        else:\n",
    "            co_in.extend(z[1:])\n",
    "    return co_in\n",
    "\n",
    "def get_third_grp_struct():\n",
    "    ll = []\n",
    "    for x in range(3):\n",
    "        ll.append([x, x*2 + 3, x*2 + 4, x+1])\n",
    "    ll[-1][-1] = 0\n",
    "    co_in = []\n",
    "    for i,z in enumerate(ll):\n",
    "        if i == 0:\n",
    "            co_in.extend(z)\n",
    "        else:\n",
    "            co_in.extend(z[1:])\n",
    "    return co_in\n",
    "    \n",
    "def get_second_grp_struct():\n",
    "    ll = []\n",
    "    for x in range(6):\n",
    "        ll.append([x, x*2 + 6, x*2 + 7, x+1])\n",
    "    ll[-1][-1] = 0\n",
    "    co_in = []\n",
    "    for i,z in enumerate(ll):\n",
    "        if i == 0:\n",
    "            co_in.extend(z)\n",
    "        else:\n",
    "            co_in.extend(z[1:])\n",
    "    return co_in\n",
    "\n",
    "def load_samples():\n",
    "    samples_data = pickle.load(open(\"newgooddata.pkl\",\"rb\"))\n",
    "    return samples_data\n",
    "\n",
    "def get_normalized_torso_data(samples):\n",
    "    \n",
    "\n",
    "def load_componets():\n",
    "    f1 = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS1.txt', \"r\")\n",
    "    f2 = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS2.txt', \"r\")\n",
    "    f3 = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS3.txt', \"r\")\n",
    "    f4 = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS4.txt', \"r\")\n",
    "    f5 = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS5.txt', \"r\")\n",
    "    f = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS.txt', \"r\")\n",
    "\n",
    "    data = [{}, {}, {}, {}, {}, {}]\n",
    "    datal = [[], [], [], [], [], []]\n",
    "    for j, f_ in enumerate([f, f1, f2, f3, f4, f5]):\n",
    "        for i,curve in enumerate(f_):\n",
    "            data[j][i] = np.reshape(np.array([int(x) for x in curve.split()]), (-1,2))\n",
    "            datal[j].append(np.array([float((int(x)-255.5)/255.5) for x in curve.split()]).tolist())\n",
    "    \n",
    "    coord_nvs = [20, 8, 8, 8, 4, 4] # 3, 3 TODO\n",
    "    ests = []\n",
    "    for org in range(6):\n",
    "        estimator = PCA(n_components=coord_nvs[org], \n",
    "                        svd_solver='randomized').fit(np.asarray(datal[org]))\n",
    "        ests.append(estimator)\n",
    "        \n",
    "    return ests\n",
    "\n",
    "def vector_to_image(vector):\n",
    "    co_in = get_first_grp_struct()\n",
    "    co_in123 = get_second_grp_struct()\n",
    "    co_in45 = get_third_grp_struct()\n",
    "    ests = load_componets()\n",
    "    coord_or = [co_in, co_in123, co_in123, co_in123, co_in45, co_in45]\n",
    "    coord_nvs = [20, 8, 8, 8, 4, 4]\n",
    "    offset = 0\n",
    "    new_points = []\n",
    "    try:\n",
    "        for org in range(6):\n",
    "            sample_o = vector[offset : offset + coord_nvs[org]]\n",
    "            offset += coord_nvs[org]\n",
    "            co_in_o = coord_or[org]\n",
    "            curves_es_o = ests[org].mean_\n",
    "            for i,val in enumerate(sample_o):\n",
    "                curves_es_o = curves_es_o + ests[org].components_[i]*val\n",
    "            curves_es_o = np.reshape((curves_es_o*255.5 + 255.5), (-1, 2)).astype(int).tolist()\n",
    "            c = [curves_es_o[index][0] for index in co_in_o]\n",
    "            d = [curves_es_o[index][1] for index in co_in_o]\n",
    "            tck, _ = splprep([c, d], s=0.0, per=1)\n",
    "            new_points_o = splev(np.linspace(0, 1, 1000), tck)\n",
    "            new_points.append(new_points_o)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    gray_values = [230, 100, 50, 150, 200, 0]  # TODO: change values for normalization\n",
    "    img = np.ones((512,512))*255\n",
    "    for ind,organ in enumerate(new_points):  # Need to be made faster / may be graphs\n",
    "        organ = np.array(organ).astype(int)\n",
    "        img[organ[0],organ[1]] = gray_values[ind]\n",
    "        img[organ[0]+1,organ[1]] = gray_values[ind]\n",
    "        img[organ[0],organ[1]+1] = gray_values[ind]\n",
    "        img[organ[0]+1,organ[1]+1] = gray_values[ind]\n",
    "        img[organ[0]-1,organ[1]] = gray_values[ind]\n",
    "        img[organ[0],organ[1]-1] = gray_values[ind]\n",
    "        img[organ[0]-1,organ[1]-1] = gray_values[ind]\n",
    "        \n",
    "    return img\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
