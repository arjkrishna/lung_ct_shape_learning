{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from scipy.interpolate import splprep, splev\n",
    "\n",
    "def sample(logits):\n",
    "    noise1 = tf.random_uniform(tf.shape(logits))\n",
    "#     noise2 = tf.random_uniform(tf.shape(logits))\n",
    "    # the values should be random in the beginning, so should be different\n",
    "    # in the beginning before returning usually the same actions for both\n",
    "    # what if it chooses the 'do-nothing action?'\n",
    "    # maybe just use sequence of actions as different output (8 actions = 8 output)\n",
    "    # maybe don't use this framework at all and do the alternate classifier training ( seperate \n",
    "    # usual cnns) and reinforcement learning.\n",
    "    return tf.argmax(logits - tf.log(-tf.log(3*noise1)), 1)#,\n",
    "#             tf.argmax(logits - tf.log(-tf.log(noise2)), 1)\n",
    "\n",
    "\n",
    "def fc(x, scope, nh, act=tf.nn.relu, init_scale=1.0):\n",
    "    with tf.variable_scope(scope):\n",
    "        nin = x.get_shape()[1].value\n",
    "        w = tf.get_variable(\"w\", [nin, nh], initializer=ortho_init(init_scale))\n",
    "        b = tf.get_variable(\"b\", [nh], initializer=tf.constant_initializer(0.0))\n",
    "        z = tf.matmul(x, w)+b\n",
    "        h = act(z)\n",
    "        return h\n",
    "    \n",
    "def ortho_init(scale=1.0):\n",
    "    def _ortho_init(shape, dtype, partition_info=None):\n",
    "        #lasagne ortho init for tf\n",
    "        shape = tuple(shape)\n",
    "        if len(shape) == 2:\n",
    "            flat_shape = shape\n",
    "        elif len(shape) == 4: # assumes NHWC\n",
    "            flat_shape = (np.prod(shape[:-1]), shape[-1])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "        u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "        q = u if u.shape == flat_shape else v # pick the one with the correct shape\n",
    "        q = q.reshape(shape)\n",
    "        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)\n",
    "    return _ortho_init\n",
    "\n",
    "def cat_entropy(logits):\n",
    "    a0 = logits - tf.reduce_max(logits, 1, keepdims=True)\n",
    "    ea0 = tf.exp(a0)\n",
    "    z0 = tf.reduce_sum(ea0, 1, keepdims=True)\n",
    "    p0 = ea0 / z0\n",
    "    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)\n",
    "\n",
    "# def cat_entropy_softmax(p0):\n",
    "#     return - tf.reduce_sum(p0 * tf.log(p0 + 1e-6), axis = 1)\n",
    "\n",
    "def mse(pred, target):\n",
    "    return tf.square(pred-target)/2.\n",
    "\n",
    "def find_trainable_variables(key):\n",
    "    with tf.variable_scope(key):\n",
    "        return tf.trainable_variables()\n",
    "\n",
    "def discount_with_dones(rewards, dones, gamma):\n",
    "    discounted = []\n",
    "    r = 0\n",
    "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "        r = reward + gamma * r * (1. - done)  # fixed off by one bug\n",
    "        discounted.append(r)\n",
    "    return discounted[::-1]\n",
    "\n",
    "def make_path(f):\n",
    "    return os.makedirs(f, exist_ok=True)\n",
    "\n",
    "def constant(p):\n",
    "    return 1\n",
    "\n",
    "def linear(p):\n",
    "    return 1-p\n",
    "\n",
    "def my_explained_variance(qpred, q):\n",
    "    _, vary = tf.nn.moments(q, axes=[0, 1])\n",
    "    _, varpred = tf.nn.moments(q - qpred, axes=[0, 1])\n",
    "    check_shape([vary, varpred], [[]] * 2)\n",
    "    return 1.0 - (varpred / vary)\n",
    "\n",
    "def get_first_grp_struct():\n",
    "    ll = []\n",
    "    for x in range(12):\n",
    "        ll.append([x, x*2 + 12, x*2 + 13, x+1])\n",
    "    ll[-1][-1] = 0\n",
    "    co_in = []\n",
    "    for i,z in enumerate(ll):\n",
    "        if i == 0:\n",
    "            co_in.extend(z)\n",
    "        else:\n",
    "            co_in.extend(z[1:])\n",
    "    return co_in\n",
    "\n",
    "def get_third_grp_struct():\n",
    "    ll = []\n",
    "    for x in range(3):\n",
    "        ll.append([x, x*2 + 3, x*2 + 4, x+1])\n",
    "    ll[-1][-1] = 0\n",
    "    co_in = []\n",
    "    for i,z in enumerate(ll):\n",
    "        if i == 0:\n",
    "            co_in.extend(z)\n",
    "        else:\n",
    "            co_in.extend(z[1:])\n",
    "    return co_in\n",
    "    \n",
    "def get_second_grp_struct():\n",
    "    ll = []\n",
    "    for x in range(6):\n",
    "        ll.append([x, x*2 + 6, x*2 + 7, x+1])\n",
    "    ll[-1][-1] = 0\n",
    "    co_in = []\n",
    "    for i,z in enumerate(ll):\n",
    "        if i == 0:\n",
    "            co_in.extend(z)\n",
    "        else:\n",
    "            co_in.extend(z[1:])\n",
    "    return co_in\n",
    "\n",
    "def load_componets():\n",
    "    f1 = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS1.txt', \"r\")\n",
    "    f2 = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS2.txt', \"r\")\n",
    "    f3 = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS3.txt', \"r\")\n",
    "    f4 = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS4.txt', \"r\")\n",
    "    f5 = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS5.txt', \"r\")\n",
    "    f = open('/Users/arjunkrishna/CT_image_pca_visualization/data/pgnn/pgnn/BMDSXY_NODES_POS.txt', \"r\")\n",
    "\n",
    "    data = [{}, {}, {}, {}, {}, {}]\n",
    "    datal = [[], [], [], [], [], []]\n",
    "    for j, f_ in enumerate([f, f1, f2, f3, f4, f5]):\n",
    "        for i,curve in enumerate(f_):\n",
    "            data[j][i] = np.reshape(np.array([int(x) for x in curve.split()]), (-1,2))\n",
    "            datal[j].append(np.array([float((int(x)-255.5)/255.5) for x in curve.split()]).tolist())\n",
    "    \n",
    "    coord_nvs = [20, 8, 8, 8, 4, 4] # 3, 3 TODO\n",
    "    ests = []\n",
    "    for org in range(6):\n",
    "        estimator = PCA(n_components=coord_nvs[org], \n",
    "                        svd_solver='randomized').fit(np.asarray(datal[org]))\n",
    "        ests.append(estimator)\n",
    "        \n",
    "    return ests\n",
    "\n",
    "def vectors_to_images(vectors):\n",
    "    co_in = get_first_grp_struct()\n",
    "    co_in123 = get_second_grp_struct()\n",
    "    co_in45 = get_third_grp_struct()\n",
    "    ests = load_componets()\n",
    "    coord_or = [co_in, co_in123, co_in123, co_in123, co_in45, co_in45]\n",
    "    vector0 = vectors[0]\n",
    "    int_lvl = vector0[100]\n",
    "    coord_nvs = [20, 8, 8, 8, 3, 3]\n",
    "    offset = 0\n",
    "    new_points = []\n",
    "    rew = np.ones((len(vectors),) )\n",
    "    try:\n",
    "        for org in range(6):\n",
    "            if org == 0:\n",
    "                offset += coord_nvs[org]\n",
    "                continue\n",
    "            sample_o1 = vector0[offset : offset + coord_nvs[org]]\n",
    "            sample_o2 = vector0[offset + 50: offset + coord_nvs[org]]\n",
    "            if int_level == 0:\n",
    "                sample_o = 0.7*sample_o1 + 0.3*sample_o2\n",
    "            elif int_level == 1:\n",
    "                sample_o = 0.5*sample_o1 + 0.5*sample_o2\n",
    "            else:\n",
    "                sample_o = 0.3*sample_o1 + 0.7*sample_o2\n",
    "            offset += coord_nvs[org]\n",
    "            co_in_o = coord_or[org]\n",
    "            curves_es_o = ests[org].mean_\n",
    "            for i,val in enumerate(sample_o):\n",
    "                curves_es_o = curves_es_o + ests[org].components_[i]*val\n",
    "            curves_es_o = np.reshape((curves_es_o*255.5 + 255.5), (-1, 2)).astype(int).tolist()\n",
    "            c = [curves_es_o[index][0] for index in co_in_o]\n",
    "            d = [curves_es_o[index][1] for index in co_in_o]\n",
    "            tck, _ = splprep([c, d], s=0.0, per=1)\n",
    "            new_points_o = splev(np.linspace(0, 1, 1000), tck)\n",
    "            new_points.append(new_points_o)\n",
    "    except:\n",
    "        return None, rew\n",
    "    \n",
    "    gray_values = [100, 50, 150, 200, 0]  # TODO: change values for normalization\n",
    "    img = np.ones((512,512))*255\n",
    "    for ind,organ in enumerate(new_points):  # Need to be made faster / may be graphs\n",
    "        organ = np.array(organ).astype(int)\n",
    "        img[organ[0],organ[1]] = gray_values[ind]\n",
    "        img[organ[0]+1,organ[1]] = gray_values[ind]\n",
    "        img[organ[0],organ[1]+1] = gray_values[ind]\n",
    "        img[organ[0]+1,organ[1]+1] = gray_values[ind]\n",
    "        img[organ[0]-1,organ[1]] = gray_values[ind]\n",
    "        img[organ[0],organ[1]-1] = gray_values[ind]\n",
    "        img[organ[0]-1,organ[1]-1] = gray_values[ind]\n",
    "        \n",
    "    img = np.expand_dims(img, axis=0).repeat(len(vectors), 0)\n",
    "    c = -1\n",
    "    for vector, im in zip(vectors, img):\n",
    "        c+=1\n",
    "        torso = vector[101:]\n",
    "        curves_es_o = ests[0].mean_\n",
    "        for i,val in enumerate(torso):\n",
    "            curves_es_o = curves_es_o + ests[0].components_[i]*val\n",
    "        curves_es_o = np.reshape((curves_es_o*255.5 + 255.5), (-1, 2)).astype(int).tolist()\n",
    "        try:\n",
    "            c = [curves_es_o[index][0] for index in co_in]\n",
    "            d = [curves_es_o[index][1] for index in co_in]\n",
    "            tck, _ = splprep([c, d], s=0.0, per=1)\n",
    "            new_points_o = splev(np.linspace(0, 1, 1000), tck)\n",
    "            new_points_o = np.array(new_points_o).astype(int)\n",
    "        except:\n",
    "            rew[c] = 0\n",
    "            continue\n",
    "        im[new_points_o[0],new_points_o[1]] = 230\n",
    "        im[new_points_o[0]+1,new_points_o[1]] = 230\n",
    "        im[new_points_o[0],new_points_o[1]+1] = 230\n",
    "        im[new_points_o[0]+1,new_points_o[1]+1] = 230\n",
    "        im[new_points_o[0]-1,new_points_o[1]] = 230\n",
    "        im[new_points_o[0],new_points_o[1]-1] = 230\n",
    "        im[new_points_o[0]-1,new_points_o[1]-1] = 230\n",
    "    return img, rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "25\n",
      "1\n",
      "1\n",
      "1\n",
      "20\n",
      "7\n",
      "2\n",
      "13\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    noise = tf.random_uniform(tf.shape((2,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2)))\n",
    "    with tf.Session() as sess:  print(tf.argmax((2,8,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2) - tf.log(-tf.log(4*noise)), -1).eval()) \n",
    "    # noise[0].val\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 349,  228,   41, -251, -141,  -55,  355])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([41, 162, 349, 641, 531, 445, 35])\n",
    "B = np.array([42, 300, 323, 479, 436, 389, 36])\n",
    "C = np.array([1.4, 7, 14, 28, 70, 140])\n",
    "# A = np.where(A==41, 0.2, A)\n",
    "390-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.expand_dims(A, axis=0)\n",
    "B = np.expand_dims(B, axis=0).repeat(3, 0)\n",
    "A = np.repeat(A, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.append(A,B, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "a = np.random.random((3, 6, 4))\n",
    "# len(a[0])\n",
    "a[0,0] = 2.0\n",
    "a[1,0] = 0.4\n",
    "a[2,0] = 0.3\n",
    "a[3,0] = 0\n",
    "a[4,0] = 00.\n",
    "\n",
    "a[0,1] = 0.4\n",
    "a[0,2] = 0.3\n",
    "a[0,3] = 0\n",
    "a = a.tolist()\n",
    "print(\"Data = \", a)\n",
    "# a = a*20\n",
    "# print(\"Data = \", a)\n",
    "\n",
    "# normalize the data attributes\n",
    "# normalized = preprocessing.normalize(a)\n",
    "normalized2 = preprocessing.MinMaxScaler()\n",
    "a_scaled = normalized2.fit_transform(a) # fit # transform\n",
    "normalized2.inverse_transform(a_scaled),a\n",
    "# print(\"Normalized Data = \", normalized)\n",
    "print(\"Normalized Data = \", a_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
